{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:35: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=4, units=64)`\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"linear\", units=2)`\n",
      "/anaconda/lib/python3.6/site-packages/keras/models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 29.0\n",
      "Total reward: 41.0\n",
      "Total reward: 22.0\n",
      "Total reward: 13.0\n",
      "Total reward: 16.0\n",
      "Total reward: 13.0\n",
      "Total reward: 17.0\n",
      "Total reward: 26.0\n",
      "Total reward: 18.0\n",
      "Total reward: 16.0\n",
      "Total reward: 16.0\n",
      "Total reward: 13.0\n",
      "Total reward: 15.0\n",
      "Total reward: 12.0\n",
      "Total reward: 15.0\n",
      "Total reward: 13.0\n",
      "Total reward: 12.0\n",
      "Total reward: 16.0\n",
      "Total reward: 25.0\n",
      "Total reward: 18.0\n",
      "Total reward: 11.0\n",
      "Total reward: 11.0\n",
      "Total reward: 8.0\n",
      "Total reward: 11.0\n",
      "Total reward: 45.0\n",
      "Total reward: 8.0\n",
      "Total reward: 12.0\n",
      "Total reward: 19.0\n",
      "Total reward: 9.0\n",
      "Total reward: 9.0\n",
      "Total reward: 12.0\n",
      "Total reward: 12.0\n",
      "Total reward: 27.0\n",
      "Total reward: 12.0\n",
      "Total reward: 11.0\n",
      "Total reward: 10.0\n",
      "Total reward: 15.0\n",
      "Total reward: 11.0\n",
      "Total reward: 9.0\n",
      "Total reward: 12.0\n",
      "Total reward: 27.0\n",
      "Total reward: 12.0\n",
      "Total reward: 18.0\n",
      "Total reward: 10.0\n",
      "Total reward: 9.0\n",
      "Total reward: 17.0\n",
      "Total reward: 11.0\n",
      "Total reward: 13.0\n",
      "Total reward: 10.0\n",
      "Total reward: 12.0\n",
      "Total reward: 14.0\n",
      "Total reward: 18.0\n",
      "Total reward: 12.0\n",
      "Total reward: 12.0\n",
      "Total reward: 10.0\n",
      "Total reward: 11.0\n",
      "Total reward: 15.0\n",
      "Total reward: 10.0\n",
      "Total reward: 11.0\n",
      "Total reward: 14.0\n",
      "Total reward: 12.0\n",
      "Total reward: 9.0\n",
      "Total reward: 10.0\n",
      "Total reward: 12.0\n",
      "Total reward: 13.0\n",
      "Total reward: 11.0\n",
      "Total reward: 10.0\n",
      "Total reward: 11.0\n",
      "Total reward: 11.0\n",
      "Total reward: 12.0\n",
      "Total reward: 12.0\n",
      "Total reward: 11.0\n",
      "Total reward: 15.0\n",
      "Total reward: 10.0\n",
      "Total reward: 15.0\n",
      "Total reward: 9.0\n",
      "Total reward: 12.0\n",
      "Total reward: 9.0\n",
      "Total reward: 12.0\n",
      "Total reward: 12.0\n",
      "Total reward: 10.0\n",
      "Total reward: 9.0\n",
      "Total reward: 9.0\n",
      "Total reward: 15.0\n",
      "Total reward: 14.0\n",
      "Total reward: 8.0\n",
      "Total reward: 11.0\n",
      "Total reward: 10.0\n",
      "Total reward: 16.0\n",
      "Total reward: 9.0\n",
      "Total reward: 11.0\n",
      "Total reward: 10.0\n",
      "Total reward: 9.0\n",
      "Total reward: 17.0\n",
      "Total reward: 10.0\n",
      "Total reward: 9.0\n",
      "Total reward: 11.0\n",
      "Total reward: 11.0\n",
      "Total reward: 16.0\n",
      "Total reward: 9.0\n",
      "Total reward: 10.0\n",
      "Total reward: 11.0\n",
      "Total reward: 14.0\n",
      "Total reward: 12.0\n",
      "Total reward: 14.0\n",
      "Total reward: 11.0\n",
      "Total reward: 15.0\n",
      "Total reward: 8.0\n",
      "Total reward: 8.0\n",
      "Total reward: 10.0\n",
      "Total reward: 10.0\n",
      "Total reward: 8.0\n",
      "Total reward: 9.0\n",
      "Total reward: 10.0\n",
      "Total reward: 9.0\n",
      "Total reward: 11.0\n",
      "Total reward: 10.0\n",
      "Total reward: 11.0\n",
      "Total reward: 9.0\n",
      "Total reward: 13.0\n",
      "Total reward: 10.0\n",
      "Total reward: 10.0\n",
      "Total reward: 12.0\n",
      "Total reward: 10.0\n",
      "Total reward: 9.0\n",
      "Total reward: 10.0\n",
      "Total reward: 18.0\n",
      "Total reward: 11.0\n",
      "Total reward: 11.0\n",
      "Total reward: 12.0\n",
      "Total reward: 12.0\n",
      "Total reward: 10.0\n",
      "Total reward: 11.0\n",
      "Total reward: 9.0\n",
      "Total reward: 10.0\n",
      "Total reward: 9.0\n",
      "Total reward: 12.0\n",
      "Total reward: 14.0\n",
      "Total reward: 10.0\n",
      "Total reward: 9.0\n",
      "Total reward: 10.0\n",
      "Total reward: 8.0\n",
      "Total reward: 11.0\n",
      "Total reward: 9.0\n",
      "Total reward: 12.0\n",
      "Total reward: 12.0\n",
      "Total reward: 11.0\n",
      "Total reward: 9.0\n",
      "Total reward: 10.0\n",
      "Total reward: 10.0\n",
      "Total reward: 12.0\n",
      "Total reward: 11.0\n",
      "Total reward: 9.0\n",
      "Total reward: 11.0\n",
      "Total reward: 10.0\n",
      "Total reward: 10.0\n",
      "Total reward: 9.0\n",
      "Total reward: 10.0\n",
      "Total reward: 8.0\n",
      "Total reward: 8.0\n",
      "Total reward: 11.0\n",
      "Total reward: 10.0\n",
      "Total reward: 11.0\n",
      "Total reward: 13.0\n",
      "Total reward: 10.0\n",
      "Total reward: 9.0\n",
      "Total reward: 9.0\n",
      "Total reward: 8.0\n",
      "Total reward: 14.0\n",
      "Total reward: 12.0\n",
      "Total reward: 13.0\n",
      "Total reward: 10.0\n",
      "Total reward: 10.0\n",
      "Total reward: 10.0\n",
      "Total reward: 12.0\n",
      "Total reward: 12.0\n",
      "Total reward: 12.0\n",
      "Total reward: 13.0\n",
      "Total reward: 10.0\n",
      "Total reward: 10.0\n",
      "Total reward: 12.0\n",
      "Total reward: 9.0\n",
      "Total reward: 11.0\n",
      "Total reward: 11.0\n",
      "Total reward: 10.0\n",
      "Total reward: 9.0\n",
      "Total reward: 10.0\n",
      "Total reward: 11.0\n",
      "Total reward: 10.0\n",
      "Total reward: 11.0\n",
      "Total reward: 12.0\n",
      "Total reward: 10.0\n",
      "Total reward: 11.0\n",
      "Total reward: 11.0\n",
      "Total reward: 11.0\n",
      "Total reward: 14.0\n",
      "Total reward: 11.0\n",
      "Total reward: 11.0\n",
      "Total reward: 14.0\n",
      "Total reward: 18.0\n",
      "Total reward: 12.0\n",
      "Total reward: 12.0\n",
      "Total reward: 15.0\n",
      "Total reward: 14.0\n",
      "Total reward: 14.0\n",
      "Total reward: 10.0\n",
      "Total reward: 14.0\n",
      "Total reward: 11.0\n",
      "Total reward: 15.0\n",
      "Total reward: 18.0\n",
      "Total reward: 14.0\n",
      "Total reward: 14.0\n",
      "Total reward: 16.0\n",
      "Total reward: 15.0\n",
      "Total reward: 13.0\n",
      "Total reward: 15.0\n",
      "Total reward: 16.0\n",
      "Total reward: 15.0\n",
      "Total reward: 16.0\n",
      "Total reward: 17.0\n",
      "Total reward: 13.0\n",
      "Total reward: 17.0\n",
      "Total reward: 17.0\n",
      "Total reward: 18.0\n",
      "Total reward: 19.0\n",
      "Total reward: 20.0\n",
      "Total reward: 19.0\n",
      "Total reward: 15.0\n",
      "Total reward: 12.0\n",
      "Total reward: 21.0\n",
      "Total reward: 20.0\n",
      "Total reward: 23.0\n",
      "Total reward: 19.0\n",
      "Total reward: 19.0\n",
      "Total reward: 19.0\n",
      "Total reward: 27.0\n",
      "Total reward: 21.0\n",
      "Total reward: 19.0\n",
      "Total reward: 26.0\n",
      "Total reward: 30.0\n",
      "Total reward: 22.0\n",
      "Total reward: 27.0\n",
      "Total reward: 18.0\n",
      "Total reward: 24.0\n",
      "Total reward: 30.0\n",
      "Total reward: 20.0\n",
      "Total reward: 23.0\n",
      "Total reward: 22.0\n",
      "Total reward: 23.0\n",
      "Total reward: 36.0\n",
      "Total reward: 36.0\n",
      "Total reward: 30.0\n",
      "Total reward: 44.0\n",
      "Total reward: 50.0\n",
      "Total reward: 97.0\n",
      "Total reward: 119.0\n",
      "Total reward: 134.0\n",
      "Total reward: 160.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2b6453514433>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cartpole-basic.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-2b6453514433>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-2b6453514433>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mGAMMA\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial)\u001b[0m\n\u001b[1;32m   2332\u001b[0m     \"\"\"\n\u001b[1;32m   2333\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out, keepdims=keepdims,\n\u001b[0;32m-> 2334\u001b[0;31m                           initial=initial)\n\u001b[0m\u001b[1;32m   2335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# OpenGym CartPole-v0\n",
    "# -------------------\n",
    "#\n",
    "# This code demonstrates use of a basic Q-network (without target network)\n",
    "# to solve OpenGym CartPole-v0 problem.\n",
    "#\n",
    "# Made as part of blog series Let's make a DQN, available at: \n",
    "# https://jaromiru.com/2016/10/03/lets-make-a-dqn-implementation/\n",
    "# \n",
    "# author: Jaromir Janisch, 2016\n",
    "\n",
    "\n",
    "#--- enable this to run on GPU\n",
    "# import os    \n",
    "# os.environ['THEANO_FLAGS'] = \"device=gpu,floatX=float32\"  \n",
    "\n",
    "import random, numpy, math, gym\n",
    "\n",
    "#-------------------- BRAIN ---------------------------\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self, stateCnt, actionCnt):\n",
    "        self.stateCnt = stateCnt\n",
    "        self.actionCnt = actionCnt\n",
    "\n",
    "        self.model = self._createModel()\n",
    "        # self.model.load_weights(\"cartpole-basic.h5\")\n",
    "\n",
    "    def _createModel(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(output_dim=64, activation='relu', input_dim=stateCnt))\n",
    "        model.add(Dense(output_dim=actionCnt, activation='linear'))\n",
    "\n",
    "        opt = RMSprop(lr=0.00025)\n",
    "        model.compile(loss='mse', optimizer=opt)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(self, x, y, epoch=1, verbose=0):\n",
    "        self.model.fit(x, y, batch_size=64, nb_epoch=epoch, verbose=verbose)\n",
    "\n",
    "    def predict(self, s):\n",
    "        return self.model.predict(s)\n",
    "\n",
    "    def predictOne(self, s):\n",
    "        return self.predict(s.reshape(1, self.stateCnt)).flatten()\n",
    "\n",
    "#-------------------- MEMORY --------------------------\n",
    "class Memory:   # stored as ( s, a, r, s_ )\n",
    "    samples = []\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def add(self, sample):\n",
    "        self.samples.append(sample)        \n",
    "\n",
    "        if len(self.samples) > self.capacity:\n",
    "            self.samples.pop(0)\n",
    "\n",
    "    def sample(self, n):\n",
    "        n = min(n, len(self.samples))\n",
    "        return random.sample(self.samples, n)\n",
    "\n",
    "#-------------------- AGENT ---------------------------\n",
    "MEMORY_CAPACITY = 100000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "MAX_EPSILON = 1\n",
    "MIN_EPSILON = 0.01\n",
    "LAMBDA = 0.001      # speed of decay\n",
    "\n",
    "class Agent:\n",
    "    steps = 0\n",
    "    epsilon = MAX_EPSILON\n",
    "\n",
    "    def __init__(self, stateCnt, actionCnt):\n",
    "        self.stateCnt = stateCnt\n",
    "        self.actionCnt = actionCnt\n",
    "\n",
    "        self.brain = Brain(stateCnt, actionCnt)\n",
    "        self.memory = Memory(MEMORY_CAPACITY)\n",
    "        \n",
    "    def act(self, s):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.actionCnt-1)\n",
    "        else:\n",
    "            return numpy.argmax(self.brain.predictOne(s))\n",
    "\n",
    "    def observe(self, sample):  # in (s, a, r, s_) format\n",
    "        self.memory.add(sample)        \n",
    "\n",
    "        # slowly decrease Epsilon based on our eperience\n",
    "        self.steps += 1\n",
    "        self.epsilon = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * math.exp(-LAMBDA * self.steps)\n",
    "\n",
    "    def replay(self):    \n",
    "        batch = self.memory.sample(BATCH_SIZE)\n",
    "        batchLen = len(batch)\n",
    "\n",
    "        no_state = numpy.zeros(self.stateCnt)\n",
    "\n",
    "        states = numpy.array([ o[0] for o in batch ])\n",
    "        states_ = numpy.array([ (no_state if o[3] is None else o[3]) for o in batch ])\n",
    "\n",
    "        p = self.brain.predict(states)\n",
    "        p_ = self.brain.predict(states_)\n",
    "\n",
    "        x = numpy.zeros((batchLen, self.stateCnt))\n",
    "        y = numpy.zeros((batchLen, self.actionCnt))\n",
    "        \n",
    "        for i in range(batchLen):\n",
    "            o = batch[i]\n",
    "            s = o[0]; a = o[1]; r = o[2]; s_ = o[3]\n",
    "            \n",
    "            t = p[i]\n",
    "            if s_ is None:\n",
    "                t[a] = r\n",
    "            else:\n",
    "                t[a] = r + GAMMA * numpy.amax(p_[i])\n",
    "\n",
    "            x[i] = s\n",
    "            y[i] = t\n",
    "\n",
    "        self.brain.train(x, y)\n",
    "\n",
    "#-------------------- ENVIRONMENT ---------------------\n",
    "class Environment:\n",
    "    def __init__(self, problem):\n",
    "        self.problem = problem\n",
    "        self.env = gym.make(problem)\n",
    "\n",
    "    def run(self, agent):\n",
    "        s = self.env.reset()\n",
    "        R = 0 \n",
    "\n",
    "        while True:            \n",
    "            #self.env.render()\n",
    "\n",
    "            a = agent.act(s)\n",
    "\n",
    "            s_, r, done, info = self.env.step(a)\n",
    "\n",
    "            if done: # terminal state\n",
    "                s_ = None\n",
    "\n",
    "            agent.observe( (s, a, r, s_) )\n",
    "            agent.replay()            \n",
    "\n",
    "            s = s_\n",
    "            R += r\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print(\"Total reward:\", R)\n",
    "\n",
    "#-------------------- MAIN ----------------------------\n",
    "PROBLEM = 'CartPole-v0'\n",
    "env = Environment(PROBLEM)\n",
    "\n",
    "stateCnt  = env.env.observation_space.shape[0]\n",
    "actionCnt = env.env.action_space.n\n",
    "\n",
    "agent = Agent(stateCnt, actionCnt)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        env.run(agent)\n",
    "finally:\n",
    "    agent.brain.model.save(\"cartpole-basic.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
