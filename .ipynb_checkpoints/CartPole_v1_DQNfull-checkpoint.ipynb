{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make two updates to our basic dqn: \n",
    "*  add a target network \n",
    "*  add reward clipping \n",
    "\n",
    "We use logcosh loss instead of huber loss function because it is easy to use in Keras. The two are  almost the same for low delta values (of huber loss). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the environment \n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_neural_network(): \n",
    "    n_actions = env.action_space.n\n",
    "    input_dim = env.observation_space.shape[0]\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim = input_dim , activation = 'relu'))\n",
    "    model.add(Dense(32, activation = 'relu'))\n",
    "    model.add(Dense(n_actions, activation = 'linear'))\n",
    "    # Add the huber/logcosh loss function \n",
    "    model.compile(optimizer=Adam(), loss = 'logcosh')\n",
    "    return model \n",
    "\n",
    "## Neural net 1: network that approximates the q-value \n",
    "nnet_q = make_neural_network()\n",
    "## Neural net 2: target network, updated periodocally with values from nnet 1  \n",
    "nnet_target = make_neural_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replay(replay_memory, minibatch_size=32):\n",
    "    minibatch = np.random.choice(replay_memory, minibatch_size, replace=True)\n",
    "    s_l =      np.array(list(map(lambda x: x['s'], minibatch)))\n",
    "    a_l =      np.array(list(map(lambda x: x['a'], minibatch)))\n",
    "    r_l =      np.array(list(map(lambda x: x['r'], minibatch)))\n",
    "    sprime_l = np.array(list(map(lambda x: x['sprime'], minibatch)))\n",
    "    done_l   = np.array(list(map(lambda x: x['done'], minibatch)))\n",
    "    # Predict the next state values with target network\n",
    "    qvals_sprime_l = nnet_target.predict(sprime_l)\n",
    "    # Predict current state values with realtime q network\n",
    "    target_f = nnet_q.predict(s_l) \n",
    "    # q-update\n",
    "    for i,(s,a,r,qvals_sprime, done) in enumerate(zip(s_l,a_l,r_l,qvals_sprime_l, done_l)): \n",
    "        if not done:  target = r + gamma * np.max(qvals_sprime)\n",
    "        else:         target = r\n",
    "    target_f[i][a] = target\n",
    "    nnet_q.fit(s_l,target_f, epochs=1, verbose=0)\n",
    "    return nnet_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters \n",
    "n_episodes = 1200\n",
    "gamma = 0.99\n",
    "epsilon = 1\n",
    "minibatch_size = 32\n",
    "r_sums = []  # stores rewards of each epsiode \n",
    "replay_memory = [] # replay memory holds s, a, r, s'\n",
    "mem_max_size = 100000\n",
    "C = 1000 # update the target network after this many steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 23.0\n",
      "Total reward: 15.0\n",
      "Total reward: 13.0\n",
      "Total reward: 37.0\n",
      "Total reward: 30.0\n",
      "Total reward: 35.0\n",
      "Total reward: 28.0\n",
      "Total reward: 48.0\n",
      "Total reward: 56.0\n",
      "Total reward: 14.0\n",
      "Total reward: 16.0\n",
      "Total reward: 62.0\n",
      "Total reward: 49.0\n",
      "Total reward: 72.0\n",
      "Total reward: 203.0\n",
      "Total reward: 91.0\n",
      "Total reward: 190.0\n",
      "Total reward: 204.0\n",
      "Total reward: 180.0\n",
      "Total reward: 169.0\n",
      "Total reward: 170.0\n",
      "Total reward: 135.0\n",
      "Total reward: 144.0\n",
      "Total reward: 153.0\n",
      "Total reward: 115.0\n",
      "Total reward: 142.0\n",
      "Total reward: 108.0\n",
      "Total reward: 115.0\n",
      "Total reward: 160.0\n",
      "Total reward: 122.0\n",
      "Total reward: 137.0\n",
      "Total reward: 247.0\n",
      "Total reward: 287.0\n",
      "Total reward: 91.0\n",
      "Total reward: 164.0\n",
      "Total reward: 162.0\n",
      "Total reward: 73.0\n",
      "Total reward: 128.0\n",
      "Total reward: 67.0\n",
      "Total reward: 44.0\n",
      "Total reward: 168.0\n",
      "Total reward: 159.0\n",
      "Total reward: 154.0\n",
      "Total reward: 150.0\n",
      "Total reward: 93.0\n",
      "Total reward: 82.0\n",
      "Total reward: 69.0\n",
      "Total reward: 61.0\n",
      "Total reward: 121.0\n",
      "Total reward: 64.0\n",
      "Total reward: 67.0\n",
      "Total reward: 101.0\n",
      "Total reward: 51.0\n",
      "Total reward: 64.0\n",
      "Total reward: 91.0\n",
      "Total reward: 98.0\n",
      "Total reward: 74.0\n",
      "Total reward: 90.0\n",
      "Total reward: 81.0\n",
      "Total reward: 100.0\n",
      "Total reward: 112.0\n",
      "Total reward: 142.0\n",
      "Total reward: 97.0\n",
      "Total reward: 98.0\n",
      "Total reward: 104.0\n",
      "Total reward: 142.0\n",
      "Total reward: 71.0\n",
      "Total reward: 204.0\n",
      "Total reward: 83.0\n",
      "Total reward: 86.0\n",
      "Total reward: 93.0\n",
      "Total reward: 80.0\n",
      "Total reward: 108.0\n",
      "Total reward: 20.0\n",
      "Total reward: 218.0\n",
      "Total reward: 217.0\n",
      "Total reward: 146.0\n",
      "Total reward: 45.0\n",
      "Total reward: 82.0\n",
      "Total reward: 115.0\n",
      "Total reward: 78.0\n",
      "Total reward: 96.0\n",
      "Total reward: 239.0\n",
      "Total reward: 131.0\n",
      "Total reward: 85.0\n",
      "Total reward: 66.0\n",
      "Total reward: 92.0\n",
      "Total reward: 55.0\n",
      "Total reward: 77.0\n",
      "Total reward: 75.0\n",
      "Total reward: 87.0\n",
      "Total reward: 313.0\n",
      "Total reward: 146.0\n",
      "Total reward: 48.0\n",
      "Total reward: 172.0\n",
      "Total reward: 172.0\n",
      "Total reward: 150.0\n",
      "Total reward: 175.0\n",
      "Total reward: 163.0\n",
      "Total reward: 121.0\n",
      "Total reward: 84.0\n",
      "Total reward: 91.0\n",
      "Total reward: 16.0\n",
      "Total reward: 22.0\n",
      "Total reward: 81.0\n",
      "Total reward: 82.0\n",
      "Total reward: 86.0\n",
      "Total reward: 78.0\n",
      "Total reward: 70.0\n",
      "Total reward: 89.0\n",
      "Total reward: 90.0\n",
      "Total reward: 89.0\n",
      "Total reward: 17.0\n",
      "Total reward: 73.0\n",
      "Total reward: 89.0\n",
      "Total reward: 17.0\n",
      "Total reward: 26.0\n",
      "Total reward: 83.0\n",
      "Total reward: 32.0\n",
      "Total reward: 71.0\n",
      "Total reward: 90.0\n",
      "Total reward: 75.0\n",
      "Total reward: 20.0\n",
      "Total reward: 84.0\n",
      "Total reward: 34.0\n",
      "Total reward: 132.0\n",
      "Total reward: 86.0\n",
      "Total reward: 87.0\n",
      "Total reward: 90.0\n",
      "Total reward: 126.0\n",
      "Total reward: 35.0\n",
      "Total reward: 83.0\n",
      "Total reward: 17.0\n",
      "Total reward: 103.0\n",
      "Total reward: 90.0\n",
      "Total reward: 160.0\n",
      "Total reward: 318.0\n",
      "Total reward: 19.0\n",
      "Total reward: 22.0\n",
      "Total reward: 70.0\n",
      "Total reward: 21.0\n",
      "Total reward: 39.0\n",
      "Total reward: 17.0\n",
      "Total reward: 35.0\n",
      "Total reward: 18.0\n",
      "Total reward: 38.0\n",
      "Total reward: 28.0\n",
      "Total reward: 19.0\n",
      "Total reward: 19.0\n",
      "Total reward: 27.0\n",
      "Total reward: 58.0\n",
      "Total reward: 25.0\n",
      "Total reward: 29.0\n",
      "Total reward: 29.0\n",
      "Total reward: 23.0\n",
      "Total reward: 43.0\n",
      "Total reward: 32.0\n",
      "Total reward: 57.0\n",
      "Total reward: 20.0\n",
      "Total reward: 27.0\n",
      "Total reward: 57.0\n",
      "Total reward: 20.0\n",
      "Total reward: 18.0\n",
      "Total reward: 30.0\n",
      "Total reward: 38.0\n",
      "Total reward: 48.0\n",
      "Total reward: 114.0\n",
      "Total reward: 95.0\n",
      "Total reward: 44.0\n",
      "Total reward: 113.0\n",
      "Total reward: 75.0\n",
      "Total reward: 27.0\n",
      "Total reward: 35.0\n",
      "Total reward: 34.0\n",
      "Total reward: 36.0\n",
      "Total reward: 31.0\n",
      "Total reward: 111.0\n",
      "Total reward: 116.0\n",
      "Total reward: 116.0\n",
      "Total reward: 57.0\n",
      "Total reward: 137.0\n",
      "Total reward: 128.0\n",
      "Total reward: 96.0\n",
      "Total reward: 100.0\n",
      "Total reward: 89.0\n",
      "Total reward: 120.0\n",
      "Total reward: 99.0\n",
      "Total reward: 139.0\n",
      "Total reward: 105.0\n",
      "Total reward: 38.0\n"
     ]
    }
   ],
   "source": [
    "steps = 0 \n",
    "for n in range(n_episodes): \n",
    "    s = env.reset()\n",
    "    done=False\n",
    "    r_sum = 0\n",
    "    while not done: \n",
    "        # Uncomment this to see the agent learning\n",
    "        env.render()\n",
    "        # Feedforward pass for current state to get predicted q-values for all actions \n",
    "        qvals_s = nnet_q.predict(s.reshape(1,4))\n",
    "        # Choose action to be epsilon-greedy\n",
    "        if np.random.random() < epsilon:  a = env.action_space.sample()\n",
    "        else:                             a = np.argmax(qvals_s); \n",
    "        # Take step, store results \n",
    "        sprime, r, done, info = env.step(a)\n",
    "        r_sum += r \n",
    "        # add to memory, respecting memory buffer limit \n",
    "        if len(replay_memory) > mem_max_size:\n",
    "            replay_memory.pop(0)\n",
    "        replay_memory.append({\"s\":s,\"a\":a,\"r\":r,\"sprime\":sprime,\"done\":done})\n",
    "        # Update target weights every C steps \n",
    "        steps +=1 \n",
    "        if steps % C == 0: nnet_target.set_weights(nnet_q.get_weights())\n",
    "            \n",
    "        # Update state\n",
    "        s=sprime\n",
    "        \n",
    "        # Train the nnet that approximates q(s,a), using the replay memory\n",
    "        nnet_q = replay(replay_memory, minibatch_size = minibatch_size)\n",
    "        # Decrease epsilon until we hit a target threshold \n",
    "        if epsilon > 0.01:      epsilon -= 0.001\n",
    "    print(\"Total reward:\", r_sum)\n",
    "    r_sums.append(r_sum)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_sum.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
