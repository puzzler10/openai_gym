{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Taxi Problem  \n",
    "\n",
    "from \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\"\n",
    "by Tom Dietterich\n",
    "\n",
    "\n",
    "**Description**   \n",
    "There are four designated locations in the grid world indicated by R(ed), B(lue), G(reen), and Y(ellow). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drive to the passenger's location, pick up the passenger, drive to the passenger's destination (another one of the four specified locations), and then drop off the passenger. Once the passenger is dropped off, the episode ends.\n",
    "\n",
    "**Observations**   \n",
    "There are 500 discrete actions since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is the taxi), and 4 destination locations. \n",
    "\n",
    "\n",
    "**Actions**   \n",
    "There are 6 discrete deterministic actions:\n",
    "- 0: move south\n",
    "- 1: move north\n",
    "- 2: move east \n",
    "- 3: move west \n",
    "- 4: pickup passenger\n",
    "- 5: dropoff passenger\n",
    "\n",
    "**Rewards**   \n",
    "    There is a reward of -1 for each action and an additional reward of +20 for delievering the passenger. There is a reward of -10 for executing actions \"pickup\" and \"dropoff\" illegally.\n",
    "\n",
    "**Rendering**\n",
    "- blue: passenger\n",
    "- magenta: destination\n",
    "- yellow: empty taxi\n",
    "- green: full taxi\n",
    "- other letters: locations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# For animation \n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent: \n",
    "    def __init__(self, method, start_alpha = 0.1, start_gamma = 0.9, start_epsilon = 0.1):\n",
    "        \"\"\"method: one of 'q_learning', 'sarsa' or 'expected_sarsa' \"\"\"\n",
    "        self.method = method\n",
    "        self.env = gym.make('Taxi-v2')\n",
    "        self.n_squares = 25 \n",
    "        self.n_passenger_locs = 5 \n",
    "        self.n_dropoffs = 4 \n",
    "        self.n_actions = self.env.action_space.n\n",
    "        self.epsilon = start_epsilon\n",
    "        self.gamma = start_gamma\n",
    "        self.alpha = start_alpha\n",
    "        # Set up initial q-table \n",
    "        self.q = np.zeros(shape = (self.n_squares*self.n_passenger_locs*self.n_dropoffs, self.env.action_space.n))\n",
    "        # Set up policy pi, init as equiprobable random policy\n",
    "        self.pi = np.zeros_like(self.q)\n",
    "        for i in range(self.pi.shape[0]): \n",
    "            for a in range(self.n_actions): \n",
    "                self.pi[i,a] = 1/self.n_actions\n",
    "        \n",
    "    def simulate_episode(self):\n",
    "        s = self.env.reset()\n",
    "        done = False\n",
    "        r_sum = 0 \n",
    "        n_steps = 0 \n",
    "        gam = self.gamma\n",
    "        while not done: \n",
    "            n_steps += 1\n",
    "            # take action from policy\n",
    "            a = np.argmax(np.cumsum(self.pi[s,:]) > np.random.random()) \n",
    "            # take step \n",
    "            s_prime,r,done,info = self.env.step(a)    \n",
    "            if self.method == 'q_learning': \n",
    "                a_prime = np.random.choice(np.where(self.q[s_prime] == max(self.q[s_prime]))[0])\n",
    "                self.q[s,a] = self.q[s,a] + self.alpha * \\\n",
    "                    (r + gam*self.q[s_prime,a_prime] - self.q[s,a])\n",
    "            elif self.method == 'sarsa': \n",
    "                a_prime = np.argmax(np.cumsum(self.pi[s_prime,:]) > np.random.random())\n",
    "                self.q[s,a] = self.q[s,a] + self.alpha * \\\n",
    "                    (r + gam*self.q[s_prime,a_prime ] - self.q[s,a])\n",
    "            elif self.method == 'expected_sarsa':\n",
    "                self.q[s,a] = self.q[s,a] + self.alpha * \\\n",
    "                    (r + gam* np.dot(self.pi[s_prime,:],self.q[s_prime,:]) - self.q[s,a])\n",
    "            else: \n",
    "                raise Exception(\"Invalid method provided\")\n",
    "            # update policy\n",
    "            \n",
    "            best_a = np.argmax(self.q[s]) #np.random.choice(np.where(self.q[s] == max(self.q[s]))[0])\n",
    "            print(s, np.round(np.cumsum(self.pi[s,:]),3), np.round(self.q[s], 3) , best_a  )\n",
    "            print(self.pi[s])\n",
    "            for i in range(self.n_actions): \n",
    "                if i == best_a:      self.pi[s,i] = 1 - (self.n_actions-1)*(self.epsilon / self.n_actions)\n",
    "                else:                self.pi[s,i] = self.epsilon / self.n_actions\n",
    "            print(self.pi[s])\n",
    "            # decay gamma close to the end of the episode\n",
    "            if n_steps > 185: \n",
    "                gam *= 0.875\n",
    "           \n",
    "            s = s_prime\n",
    "            r_sum += r\n",
    "        return r_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_agent(agent, n_episodes= 100001, epsilon_decay = 0.999995, alpha_decay = 0.99999, print_trace = False):\n",
    "    r_sums = []\n",
    "    for ep in range(n_episodes): \n",
    "        r_sum = agent.simulate_episode()\n",
    "        # decrease epsilon and learning rate \n",
    "        agent.epsilon *= epsilon_decay\n",
    "        agent.alpha *= alpha_decay\n",
    "        if print_trace: \n",
    "            if ep % 2000 == 0 and ep > 0 : \n",
    "                print(\"Episode:\", ep, \"alpha:\", np.round(agent.alpha, 3), \"epsilon:\",  np.round(agent.epsilon, 3))\n",
    "                print (\"Last 100 episodes avg reward: \", np.mean(r_sums[ep-100:ep]))\n",
    "        r_sums.append(r_sum)\n",
    "    return r_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create agents \n",
    "sarsa_agent = Agent(method='sarsa')\n",
    "e_sarsa_agent = Agent(method='expected_sarsa')\n",
    "q_learning_agent = Agent(method='q_learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2000 alpha: 0.098 epsilon: 0.099\n",
      "Last 100 episodes avg reward:  3.45\n",
      "Episode: 4000 alpha: 0.096 epsilon: 0.098\n",
      "Last 100 episodes avg reward:  4.05\n",
      "Episode: 6000 alpha: 0.094 epsilon: 0.097\n",
      "Last 100 episodes avg reward:  1.87\n",
      "Episode: 8000 alpha: 0.092 epsilon: 0.096\n",
      "Last 100 episodes avg reward:  2.52\n",
      "Episode: 10000 alpha: 0.09 epsilon: 0.095\n",
      "Last 100 episodes avg reward:  3.56\n",
      "Episode: 12000 alpha: 0.089 epsilon: 0.094\n",
      "Last 100 episodes avg reward:  2.95\n",
      "Episode: 14000 alpha: 0.087 epsilon: 0.093\n",
      "Last 100 episodes avg reward:  3.36\n",
      "Episode: 16000 alpha: 0.085 epsilon: 0.092\n",
      "Last 100 episodes avg reward:  4.2\n",
      "Episode: 18000 alpha: 0.084 epsilon: 0.091\n",
      "Last 100 episodes avg reward:  4.43\n",
      "Episode: 20000 alpha: 0.082 epsilon: 0.09\n",
      "Last 100 episodes avg reward:  3.84\n",
      "Episode: 22000 alpha: 0.08 epsilon: 0.09\n",
      "Last 100 episodes avg reward:  2.87\n",
      "Episode: 24000 alpha: 0.079 epsilon: 0.089\n",
      "Last 100 episodes avg reward:  3.95\n",
      "Episode: 26000 alpha: 0.077 epsilon: 0.088\n",
      "Last 100 episodes avg reward:  2.3\n",
      "Episode: 28000 alpha: 0.076 epsilon: 0.087\n",
      "Last 100 episodes avg reward:  4.19\n",
      "Episode: 30000 alpha: 0.074 epsilon: 0.086\n",
      "Last 100 episodes avg reward:  4.16\n",
      "Episode: 32000 alpha: 0.073 epsilon: 0.085\n",
      "Last 100 episodes avg reward:  3.76\n",
      "Episode: 34000 alpha: 0.071 epsilon: 0.084\n",
      "Last 100 episodes avg reward:  4.07\n",
      "Episode: 36000 alpha: 0.07 epsilon: 0.084\n",
      "Last 100 episodes avg reward:  3.9\n",
      "Episode: 38000 alpha: 0.068 epsilon: 0.083\n",
      "Last 100 episodes avg reward:  4.42\n",
      "Episode: 40000 alpha: 0.067 epsilon: 0.082\n",
      "Last 100 episodes avg reward:  5.01\n",
      "Episode: 42000 alpha: 0.066 epsilon: 0.081\n",
      "Last 100 episodes avg reward:  3.9\n",
      "Episode: 44000 alpha: 0.064 epsilon: 0.08\n",
      "Last 100 episodes avg reward:  3.74\n",
      "Episode: 46000 alpha: 0.063 epsilon: 0.079\n",
      "Last 100 episodes avg reward:  5.33\n",
      "Episode: 48000 alpha: 0.062 epsilon: 0.079\n",
      "Last 100 episodes avg reward:  3.25\n",
      "Episode: 50000 alpha: 0.061 epsilon: 0.078\n",
      "Last 100 episodes avg reward:  3.7\n",
      "Episode: 52000 alpha: 0.059 epsilon: 0.077\n",
      "Last 100 episodes avg reward:  4.42\n",
      "Episode: 54000 alpha: 0.058 epsilon: 0.076\n",
      "Last 100 episodes avg reward:  4.86\n",
      "Episode: 56000 alpha: 0.057 epsilon: 0.076\n",
      "Last 100 episodes avg reward:  3.41\n",
      "Episode: 58000 alpha: 0.056 epsilon: 0.075\n",
      "Last 100 episodes avg reward:  4.61\n",
      "Episode: 60000 alpha: 0.055 epsilon: 0.074\n",
      "Last 100 episodes avg reward:  5.6\n",
      "Episode: 62000 alpha: 0.054 epsilon: 0.073\n",
      "Last 100 episodes avg reward:  5.04\n",
      "Episode: 64000 alpha: 0.053 epsilon: 0.073\n",
      "Last 100 episodes avg reward:  5.54\n",
      "Episode: 66000 alpha: 0.052 epsilon: 0.072\n",
      "Last 100 episodes avg reward:  4.19\n",
      "Episode: 68000 alpha: 0.051 epsilon: 0.071\n",
      "Last 100 episodes avg reward:  5.47\n",
      "Episode: 70000 alpha: 0.05 epsilon: 0.07\n",
      "Last 100 episodes avg reward:  5.64\n",
      "Episode: 72000 alpha: 0.049 epsilon: 0.07\n",
      "Last 100 episodes avg reward:  5.27\n",
      "Episode: 74000 alpha: 0.048 epsilon: 0.069\n",
      "Last 100 episodes avg reward:  4.73\n",
      "Episode: 76000 alpha: 0.047 epsilon: 0.068\n",
      "Last 100 episodes avg reward:  4.43\n",
      "Episode: 78000 alpha: 0.046 epsilon: 0.068\n",
      "Last 100 episodes avg reward:  4.33\n",
      "Episode: 80000 alpha: 0.045 epsilon: 0.067\n",
      "Last 100 episodes avg reward:  5.69\n",
      "Episode: 82000 alpha: 0.044 epsilon: 0.066\n",
      "Last 100 episodes avg reward:  5.35\n",
      "Episode: 84000 alpha: 0.043 epsilon: 0.066\n",
      "Last 100 episodes avg reward:  4.85\n",
      "Episode: 86000 alpha: 0.042 epsilon: 0.065\n",
      "Last 100 episodes avg reward:  4.86\n",
      "Episode: 88000 alpha: 0.041 epsilon: 0.064\n",
      "Last 100 episodes avg reward:  5.06\n",
      "Episode: 90000 alpha: 0.041 epsilon: 0.064\n",
      "Last 100 episodes avg reward:  5.76\n",
      "Episode: 92000 alpha: 0.04 epsilon: 0.063\n",
      "Last 100 episodes avg reward:  5.29\n",
      "Episode: 94000 alpha: 0.039 epsilon: 0.062\n",
      "Last 100 episodes avg reward:  4.67\n",
      "Episode: 96000 alpha: 0.038 epsilon: 0.062\n",
      "Last 100 episodes avg reward:  6.18\n",
      "Episode: 98000 alpha: 0.038 epsilon: 0.061\n",
      "Last 100 episodes avg reward:  5.41\n",
      "Episode: 100000 alpha: 0.037 epsilon: 0.061\n",
      "Last 100 episodes avg reward:  4.85\n",
      "Episode: 2000 alpha: 0.098 epsilon: 0.099\n",
      "Last 100 episodes avg reward:  2.83\n",
      "Episode: 4000 alpha: 0.096 epsilon: 0.098\n",
      "Last 100 episodes avg reward:  3.53\n",
      "Episode: 6000 alpha: 0.094 epsilon: 0.097\n",
      "Last 100 episodes avg reward:  3.24\n",
      "Episode: 8000 alpha: 0.092 epsilon: 0.096\n",
      "Last 100 episodes avg reward:  2.99\n",
      "Episode: 10000 alpha: 0.09 epsilon: 0.095\n",
      "Last 100 episodes avg reward:  3.85\n",
      "Episode: 12000 alpha: 0.089 epsilon: 0.094\n",
      "Last 100 episodes avg reward:  3.57\n",
      "Episode: 14000 alpha: 0.087 epsilon: 0.093\n",
      "Last 100 episodes avg reward:  3.52\n",
      "Episode: 16000 alpha: 0.085 epsilon: 0.092\n",
      "Last 100 episodes avg reward:  4.64\n",
      "Episode: 18000 alpha: 0.084 epsilon: 0.091\n",
      "Last 100 episodes avg reward:  5.37\n",
      "Episode: 20000 alpha: 0.082 epsilon: 0.09\n",
      "Last 100 episodes avg reward:  1.34\n",
      "Episode: 22000 alpha: 0.08 epsilon: 0.09\n",
      "Last 100 episodes avg reward:  2.68\n",
      "Episode: 24000 alpha: 0.079 epsilon: 0.089\n",
      "Last 100 episodes avg reward:  4.19\n",
      "Episode: 26000 alpha: 0.077 epsilon: 0.088\n",
      "Last 100 episodes avg reward:  4.73\n",
      "Episode: 28000 alpha: 0.076 epsilon: 0.087\n",
      "Last 100 episodes avg reward:  3.85\n",
      "Episode: 30000 alpha: 0.074 epsilon: 0.086\n",
      "Last 100 episodes avg reward:  4.61\n",
      "Episode: 32000 alpha: 0.073 epsilon: 0.085\n",
      "Last 100 episodes avg reward:  2.94\n",
      "Episode: 34000 alpha: 0.071 epsilon: 0.084\n",
      "Last 100 episodes avg reward:  3.87\n",
      "Episode: 36000 alpha: 0.07 epsilon: 0.084\n",
      "Last 100 episodes avg reward:  4.42\n",
      "Episode: 38000 alpha: 0.068 epsilon: 0.083\n",
      "Last 100 episodes avg reward:  3.51\n",
      "Episode: 40000 alpha: 0.067 epsilon: 0.082\n",
      "Last 100 episodes avg reward:  3.81\n",
      "Episode: 42000 alpha: 0.066 epsilon: 0.081\n",
      "Last 100 episodes avg reward:  4.17\n",
      "Episode: 44000 alpha: 0.064 epsilon: 0.08\n",
      "Last 100 episodes avg reward:  4.9\n",
      "Episode: 46000 alpha: 0.063 epsilon: 0.079\n",
      "Last 100 episodes avg reward:  5.13\n",
      "Episode: 48000 alpha: 0.062 epsilon: 0.079\n",
      "Last 100 episodes avg reward:  3.61\n",
      "Episode: 50000 alpha: 0.061 epsilon: 0.078\n",
      "Last 100 episodes avg reward:  3.8\n",
      "Episode: 52000 alpha: 0.059 epsilon: 0.077\n",
      "Last 100 episodes avg reward:  4.86\n",
      "Episode: 54000 alpha: 0.058 epsilon: 0.076\n",
      "Last 100 episodes avg reward:  4.7\n",
      "Episode: 56000 alpha: 0.057 epsilon: 0.076\n",
      "Last 100 episodes avg reward:  5.21\n",
      "Episode: 58000 alpha: 0.056 epsilon: 0.075\n",
      "Last 100 episodes avg reward:  5.25\n",
      "Episode: 60000 alpha: 0.055 epsilon: 0.074\n",
      "Last 100 episodes avg reward:  4.56\n",
      "Episode: 62000 alpha: 0.054 epsilon: 0.073\n",
      "Last 100 episodes avg reward:  4.27\n",
      "Episode: 64000 alpha: 0.053 epsilon: 0.073\n",
      "Last 100 episodes avg reward:  4.26\n",
      "Episode: 66000 alpha: 0.052 epsilon: 0.072\n",
      "Last 100 episodes avg reward:  4.87\n",
      "Episode: 68000 alpha: 0.051 epsilon: 0.071\n",
      "Last 100 episodes avg reward:  3.64\n",
      "Episode: 70000 alpha: 0.05 epsilon: 0.07\n",
      "Last 100 episodes avg reward:  3.78\n",
      "Episode: 72000 alpha: 0.049 epsilon: 0.07\n",
      "Last 100 episodes avg reward:  4.9\n",
      "Episode: 74000 alpha: 0.048 epsilon: 0.069\n",
      "Last 100 episodes avg reward:  5.12\n",
      "Episode: 76000 alpha: 0.047 epsilon: 0.068\n",
      "Last 100 episodes avg reward:  5.59\n",
      "Episode: 78000 alpha: 0.046 epsilon: 0.068\n",
      "Last 100 episodes avg reward:  6.04\n",
      "Episode: 80000 alpha: 0.045 epsilon: 0.067\n",
      "Last 100 episodes avg reward:  4.9\n",
      "Episode: 82000 alpha: 0.044 epsilon: 0.066\n",
      "Last 100 episodes avg reward:  4.29\n",
      "Episode: 84000 alpha: 0.043 epsilon: 0.066\n",
      "Last 100 episodes avg reward:  5.54\n",
      "Episode: 86000 alpha: 0.042 epsilon: 0.065\n",
      "Last 100 episodes avg reward:  4.81\n",
      "Episode: 88000 alpha: 0.041 epsilon: 0.064\n",
      "Last 100 episodes avg reward:  4.91\n",
      "Episode: 90000 alpha: 0.041 epsilon: 0.064\n",
      "Last 100 episodes avg reward:  4.8\n",
      "Episode: 92000 alpha: 0.04 epsilon: 0.063\n",
      "Last 100 episodes avg reward:  6.72\n",
      "Episode: 94000 alpha: 0.039 epsilon: 0.062\n",
      "Last 100 episodes avg reward:  5.3\n",
      "Episode: 96000 alpha: 0.038 epsilon: 0.062\n",
      "Last 100 episodes avg reward:  6.11\n",
      "Episode: 98000 alpha: 0.038 epsilon: 0.061\n",
      "Last 100 episodes avg reward:  4.87\n",
      "Episode: 100000 alpha: 0.037 epsilon: 0.061\n",
      "Last 100 episodes avg reward:  4.29\n",
      "Episode: 2000 alpha: 0.098 epsilon: 0.099\n",
      "Last 100 episodes avg reward:  4.27\n",
      "Episode: 4000 alpha: 0.096 epsilon: 0.098\n",
      "Last 100 episodes avg reward:  4.07\n",
      "Episode: 6000 alpha: 0.094 epsilon: 0.097\n",
      "Last 100 episodes avg reward:  4.13\n",
      "Episode: 8000 alpha: 0.092 epsilon: 0.096\n",
      "Last 100 episodes avg reward:  3.19\n",
      "Episode: 10000 alpha: 0.09 epsilon: 0.095\n",
      "Last 100 episodes avg reward:  3.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 12000 alpha: 0.089 epsilon: 0.094\n",
      "Last 100 episodes avg reward:  4.02\n",
      "Episode: 14000 alpha: 0.087 epsilon: 0.093\n",
      "Last 100 episodes avg reward:  2.48\n",
      "Episode: 16000 alpha: 0.085 epsilon: 0.092\n",
      "Last 100 episodes avg reward:  2.54\n",
      "Episode: 18000 alpha: 0.084 epsilon: 0.091\n",
      "Last 100 episodes avg reward:  3.14\n",
      "Episode: 20000 alpha: 0.082 epsilon: 0.09\n",
      "Last 100 episodes avg reward:  3.32\n",
      "Episode: 22000 alpha: 0.08 epsilon: 0.09\n",
      "Last 100 episodes avg reward:  3.5\n",
      "Episode: 24000 alpha: 0.079 epsilon: 0.089\n",
      "Last 100 episodes avg reward:  2.35\n",
      "Episode: 26000 alpha: 0.077 epsilon: 0.088\n",
      "Last 100 episodes avg reward:  4.6\n",
      "Episode: 28000 alpha: 0.076 epsilon: 0.087\n",
      "Last 100 episodes avg reward:  3.66\n",
      "Episode: 30000 alpha: 0.074 epsilon: 0.086\n",
      "Last 100 episodes avg reward:  5.22\n",
      "Episode: 32000 alpha: 0.073 epsilon: 0.085\n",
      "Last 100 episodes avg reward:  4.81\n",
      "Episode: 34000 alpha: 0.071 epsilon: 0.084\n",
      "Last 100 episodes avg reward:  4.85\n",
      "Episode: 36000 alpha: 0.07 epsilon: 0.084\n",
      "Last 100 episodes avg reward:  5.09\n",
      "Episode: 38000 alpha: 0.068 epsilon: 0.083\n",
      "Last 100 episodes avg reward:  4.59\n",
      "Episode: 40000 alpha: 0.067 epsilon: 0.082\n",
      "Last 100 episodes avg reward:  4.76\n",
      "Episode: 42000 alpha: 0.066 epsilon: 0.081\n",
      "Last 100 episodes avg reward:  4.37\n",
      "Episode: 44000 alpha: 0.064 epsilon: 0.08\n",
      "Last 100 episodes avg reward:  5.18\n",
      "Episode: 46000 alpha: 0.063 epsilon: 0.079\n",
      "Last 100 episodes avg reward:  4.31\n",
      "Episode: 48000 alpha: 0.062 epsilon: 0.079\n",
      "Last 100 episodes avg reward:  5.0\n",
      "Episode: 50000 alpha: 0.061 epsilon: 0.078\n",
      "Last 100 episodes avg reward:  4.01\n",
      "Episode: 52000 alpha: 0.059 epsilon: 0.077\n",
      "Last 100 episodes avg reward:  3.81\n"
     ]
    }
   ],
   "source": [
    "# Train agents\n",
    "r_sums_sarsa = train_agent(sarsa_agent, print_trace=True)\n",
    "r_sums_e_sarsa = train_agent(e_sarsa_agent, print_trace=True)\n",
    "r_sums_q_learning = train_agent(q_learning_agent, print_trace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.DataFrame({\"Sarsa\": r_sums_sarsa, \n",
    "             \"Expected_Sarsa\": r_sums_e_sarsa, \n",
    "             \"Q-Learning\": r_sums_q_learning})\n",
    "df_ma = df.rolling(100, min_periods = 100).mean()\n",
    "df_ma.iloc[1:5000].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Expected_Sarsa    5.46\n",
       "Q-Learning        5.51\n",
       "Sarsa             5.91\n",
       "dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(df_ma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the policy for a state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent = sarsa_agent\n",
    "policy = np.zeros((5,5))\n",
    "for i in range(5): \n",
    "    for j in range(5):\n",
    "        policy[i,j] = np.argmax(agent.q[agent.env.env.encode(i,j,4,0),:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  3.,  0.,  3.,  3.],\n",
       "       [ 1.,  1.,  3.,  3.,  3.],\n",
       "       [ 1.,  1.,  1.,  3.,  3.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  3.,  1.,  3.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  3.,  0.,  3.,  3.],\n",
       "       [ 1.,  1.,  3.,  3.,  3.],\n",
       "       [ 1.,  1.,  1.,  3.,  3.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  3.,  1.,  3.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  3.,  0.,  3.,  3.],\n",
       "       [ 1.,  1.,  3.,  3.,  3.],\n",
       "       [ 1.,  1.,  1.,  3.,  3.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  3.,  1.,  3.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Viewing the final policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_frames(agent, start_state):\n",
    "    agent.env.reset()\n",
    "    agent.env.env.s = start_state\n",
    "    s = start_state\n",
    "    policy = np.argmax(agent.pi,axis =1)\n",
    "    epochs = 0\n",
    "    penalties, reward = 0, 0\n",
    "    frames = [] \n",
    "    done = False\n",
    "    frames.append({\n",
    "        'frame': agent.env.render(mode='ansi'),\n",
    "        'state': agent.env.env.s ,\n",
    "        'action': \"Start\",\n",
    "        'reward': 0\n",
    "        }\n",
    "    )\n",
    "    while not done:\n",
    "        a = policy[s]\n",
    "        s, reward, done, info = agent.env.step(a)\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        # Put each rendered frame into dict for animation\n",
    "        frames.append({\n",
    "            'frame': agent.env.render(mode='ansi'),\n",
    "            'state': s,\n",
    "            'action': a,\n",
    "            'reward': reward\n",
    "            }\n",
    "        )\n",
    "        epochs += 1\n",
    "    print(\"Timesteps taken: {}\".format(epochs))\n",
    "    print(\"Penalties incurred: {}\".format(penalties))\n",
    "    return frames\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'].getvalue())\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.4)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 16\n",
      "State: 418\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "agent = sarsa_agent\n",
    "start_state= 314\n",
    "frames = generate_frames(agent, start_state)\n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
