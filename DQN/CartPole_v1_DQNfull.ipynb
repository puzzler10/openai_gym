{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make two updates to our basic dqn: \n",
    "*  add a target network \n",
    "*  add reward clipping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "from keras import backend as K  \n",
    "import tensorflow as tf \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from math import exp \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting up the environment \n",
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def huber_loss1(y, yhat, delta = 3):\n",
    "    \"\"\"Adapted from here: https://jaromiru.com/2016/10/21/lets-make-a-dqn-full-dqn/\n",
    "    y: true value, yhat: predicted value\"\"\"\n",
    "    error = y - yhat\n",
    "    cond = K.abs(error) < delta\n",
    "    L2 = 0.5 * K.square(error)\n",
    "    L1 = delta * (K.abs(error) - 0.5 * delta)\n",
    "    loss = tf.where(cond, L2, L1)  \n",
    "    return K.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def huber_loss(a, b, in_keras=True):\n",
    "    error = a - b\n",
    "    quadratic_term = error*error / 2\n",
    "    linear_term = abs(error) - 1/2\n",
    "    use_linear_term = (abs(error) > 1.0)\n",
    "    if in_keras:\n",
    "        # Keras won't let us multiply floats by booleans, so we explicitly cast the booleans to floats\n",
    "        use_linear_term = K.cast(use_linear_term, 'float32')\n",
    "    return use_linear_term * linear_term + (1-use_linear_term) * quadratic_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_neural_network(): \n",
    "    n_actions = env.action_space.n\n",
    "    input_dim = env.observation_space.shape[0]\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim = input_dim , activation = 'relu'))\n",
    "    model.add(Dense(16, activation = 'relu'))\n",
    "    model.add(Dense(n_actions, activation = 'linear'))\n",
    "    # Add the huber/logcosh loss function \n",
    "    model.compile(optimizer=Adam(), loss = huber_loss)\n",
    "    return model \n",
    "\n",
    "## Neural net 1: network that approximates the q-value \n",
    "nnet_q = make_neural_network()\n",
    "## Neural net 2: target network, updated periodocally with values from nnet 1  \n",
    "nnet_target = make_neural_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replay(replay_memory, minibatch_size=32):\n",
    "    minibatch = np.random.choice(replay_memory, minibatch_size, replace=True)\n",
    "    s_l =      np.array(list(map(lambda x: x['s'], minibatch)))\n",
    "    a_l =      np.array(list(map(lambda x: x['a'], minibatch)))\n",
    "    r_l =      np.array(list(map(lambda x: x['r'], minibatch)))\n",
    "    sprime_l = np.array(list(map(lambda x: x['sprime'], minibatch)))\n",
    "    done_l   = np.array(list(map(lambda x: x['done'], minibatch)))\n",
    "    # Predict the next state values with target network\n",
    "    qvals_sprime_l = nnet_target.predict(sprime_l)\n",
    "    # Predict current state values with realtime q network\n",
    "    target_f = nnet_q.predict(s_l) \n",
    "    # q-update\n",
    "    for i,(s,a,r,qvals_sprime, done) in enumerate(zip(s_l,a_l,r_l,qvals_sprime_l, done_l)): \n",
    "        if not done:  target = r + gamma * np.max(qvals_sprime)\n",
    "        else:         target = r\n",
    "        target_f[i][a] = target\n",
    "    nnet_q.fit(s_l,target_f, epochs=1, verbose=0)\n",
    "    return nnet_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters \n",
    "n_episodes = 1000\n",
    "gamma = 0.99\n",
    "minibatch_size = 32\n",
    "r_sums = []  # stores rewards of each epsiode \n",
    "replay_memory = [] # replay memory holds s, a, r, s'\n",
    "mem_max_size = 100000\n",
    "min_epsilon = 0.1\n",
    "max_epsilon = 1\n",
    "C = 100 # update the target network after this many steps \n",
    "LAMBDA = 0.001 # epsilon decay parameter, capital letter to avoid clash with lambda function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n"
     ]
    }
   ],
   "source": [
    "steps = 0 \n",
    "epsilon = max_epsilon\n",
    "for n in range(n_episodes): \n",
    "    s = env.reset()\n",
    "    done=False\n",
    "    r_sum = 0\n",
    "    while not done: \n",
    "        # Uncomment this to see the agent learning\n",
    "        #env.render()\n",
    "        # Choose action to be epsilon-greedy\n",
    "        if np.random.random() < epsilon:  a = env.action_space.sample()\n",
    "        else:  \n",
    "            # Feedforward pass for current state to get predicted q-values for all actions \n",
    "            qvals_s = nnet_q.predict(s.reshape(1,4))\n",
    "            a = np.argmax(qvals_s); \n",
    "        # Take step, store results \n",
    "        sprime, r, done, info = env.step(a)\n",
    "        r_sum += r \n",
    "        # add to memory, respecting memory buffer limit \n",
    "        if len(replay_memory) > mem_max_size:\n",
    "            replay_memory.pop(0)\n",
    "        replay_memory.append({\"s\":s,\"a\":a,\"r\":r,\"sprime\":sprime,\"done\":done})\n",
    "        # Update target weights every C steps \n",
    "        steps +=1 \n",
    "        if steps % C == 0: nnet_target.set_weights(nnet_q.get_weights())\n",
    "        # Update state\n",
    "        s=sprime\n",
    "        # Train the nnet that approximates q(s,a), using the replay memory\n",
    "        nnet_q = replay(replay_memory, minibatch_size = minibatch_size)\n",
    "        # Decrease epsilon as we go\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * exp(-LAMBDA * steps )\n",
    "    #print(\"Total reward:\", r_sum)\n",
    "    r_sums.append(r_sum)\n",
    "    if n % 100 == 0: print(n)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the performance of the agent \n",
    "plt.plot(r_sums)\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rolling_average = pd.DataFrame(r_sums).rolling(100,100).mean()\n",
    "np.argmax(np.array(rolling_average > 195))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
