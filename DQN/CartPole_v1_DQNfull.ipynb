{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make two updates to our basic dqn: \n",
    "*  add a target network \n",
    "*  add reward clipping \n",
    "\n",
    "We use logcosh loss instead of huber loss function because it is easy to use in Keras. The two are  almost the same for low delta values (of huber loss). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the environment \n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_neural_network(): \n",
    "    n_actions = env.action_space.n\n",
    "    input_dim = env.observation_space.shape[0]\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim = input_dim , activation = 'relu'))\n",
    "    model.add(Dense(32, activation = 'relu'))\n",
    "    model.add(Dense(n_actions, activation = 'linear'))\n",
    "    # Add the huber/logcosh loss function \n",
    "    model.compile(optimizer=Adam(), loss = 'logcosh')\n",
    "    return model \n",
    "\n",
    "## Neural net 1: network that approximates the q-value \n",
    "nnet_q = make_neural_network()\n",
    "## Neural net 2: target network, updated periodocally with values from nnet 1  \n",
    "nnet_target = make_neural_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replay(replay_memory, minibatch_size=32):\n",
    "    minibatch = np.random.choice(replay_memory, minibatch_size, replace=True)\n",
    "    s_l =      np.array(list(map(lambda x: x['s'], minibatch)))\n",
    "    a_l =      np.array(list(map(lambda x: x['a'], minibatch)))\n",
    "    r_l =      np.array(list(map(lambda x: x['r'], minibatch)))\n",
    "    sprime_l = np.array(list(map(lambda x: x['sprime'], minibatch)))\n",
    "    done_l   = np.array(list(map(lambda x: x['done'], minibatch)))\n",
    "    # Predict the next state values with target network\n",
    "    qvals_sprime_l = nnet_target.predict(sprime_l)\n",
    "    # Predict current state values with realtime q network\n",
    "    target_f = nnet_q.predict(s_l) \n",
    "    # q-update\n",
    "    for i,(s,a,r,qvals_sprime, done) in enumerate(zip(s_l,a_l,r_l,qvals_sprime_l, done_l)): \n",
    "        if not done:  target = r + gamma * np.max(qvals_sprime)\n",
    "        else:         target = r\n",
    "    target_f[i][a] = target\n",
    "    nnet_q.fit(s_l,target_f, epochs=1, verbose=0)\n",
    "    return nnet_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters \n",
    "n_episodes = 1200\n",
    "gamma = 0.99\n",
    "epsilon = 1\n",
    "minibatch_size = 32\n",
    "r_sums = []  # stores rewards of each epsiode \n",
    "replay_memory = [] # replay memory holds s, a, r, s'\n",
    "mem_max_size = 100000\n",
    "C = 1000 # update the target network after this many steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 23.0\n",
      "Total reward: 15.0\n",
      "Total reward: 13.0\n",
      "Total reward: 37.0\n",
      "Total reward: 30.0\n",
      "Total reward: 35.0\n",
      "Total reward: 28.0\n",
      "Total reward: 48.0\n",
      "Total reward: 56.0\n",
      "Total reward: 14.0\n",
      "Total reward: 16.0\n",
      "Total reward: 62.0\n",
      "Total reward: 49.0\n",
      "Total reward: 72.0\n",
      "Total reward: 203.0\n",
      "Total reward: 91.0\n",
      "Total reward: 190.0\n",
      "Total reward: 204.0\n",
      "Total reward: 180.0\n",
      "Total reward: 169.0\n",
      "Total reward: 170.0\n",
      "Total reward: 135.0\n",
      "Total reward: 144.0\n",
      "Total reward: 153.0\n",
      "Total reward: 115.0\n",
      "Total reward: 142.0\n",
      "Total reward: 108.0\n",
      "Total reward: 115.0\n",
      "Total reward: 160.0\n",
      "Total reward: 122.0\n",
      "Total reward: 137.0\n",
      "Total reward: 247.0\n",
      "Total reward: 287.0\n",
      "Total reward: 91.0\n",
      "Total reward: 164.0\n",
      "Total reward: 162.0\n",
      "Total reward: 73.0\n",
      "Total reward: 128.0\n",
      "Total reward: 67.0\n",
      "Total reward: 44.0\n",
      "Total reward: 168.0\n",
      "Total reward: 159.0\n",
      "Total reward: 154.0\n",
      "Total reward: 150.0\n",
      "Total reward: 93.0\n",
      "Total reward: 82.0\n",
      "Total reward: 69.0\n",
      "Total reward: 61.0\n",
      "Total reward: 121.0\n",
      "Total reward: 64.0\n",
      "Total reward: 67.0\n",
      "Total reward: 101.0\n",
      "Total reward: 51.0\n",
      "Total reward: 64.0\n",
      "Total reward: 91.0\n",
      "Total reward: 98.0\n",
      "Total reward: 74.0\n",
      "Total reward: 90.0\n",
      "Total reward: 81.0\n",
      "Total reward: 100.0\n",
      "Total reward: 112.0\n",
      "Total reward: 142.0\n",
      "Total reward: 97.0\n",
      "Total reward: 98.0\n",
      "Total reward: 104.0\n",
      "Total reward: 142.0\n",
      "Total reward: 71.0\n",
      "Total reward: 204.0\n",
      "Total reward: 83.0\n",
      "Total reward: 86.0\n",
      "Total reward: 93.0\n",
      "Total reward: 80.0\n",
      "Total reward: 108.0\n",
      "Total reward: 20.0\n",
      "Total reward: 218.0\n",
      "Total reward: 217.0\n",
      "Total reward: 146.0\n",
      "Total reward: 45.0\n",
      "Total reward: 82.0\n",
      "Total reward: 115.0\n",
      "Total reward: 78.0\n",
      "Total reward: 96.0\n",
      "Total reward: 239.0\n",
      "Total reward: 131.0\n",
      "Total reward: 85.0\n",
      "Total reward: 66.0\n",
      "Total reward: 92.0\n",
      "Total reward: 55.0\n",
      "Total reward: 77.0\n",
      "Total reward: 75.0\n",
      "Total reward: 87.0\n",
      "Total reward: 313.0\n",
      "Total reward: 146.0\n",
      "Total reward: 48.0\n",
      "Total reward: 172.0\n",
      "Total reward: 172.0\n",
      "Total reward: 150.0\n",
      "Total reward: 175.0\n",
      "Total reward: 163.0\n",
      "Total reward: 121.0\n",
      "Total reward: 84.0\n",
      "Total reward: 91.0\n",
      "Total reward: 16.0\n",
      "Total reward: 22.0\n",
      "Total reward: 81.0\n",
      "Total reward: 82.0\n",
      "Total reward: 86.0\n",
      "Total reward: 78.0\n",
      "Total reward: 70.0\n",
      "Total reward: 89.0\n",
      "Total reward: 90.0\n",
      "Total reward: 89.0\n",
      "Total reward: 17.0\n",
      "Total reward: 73.0\n",
      "Total reward: 89.0\n",
      "Total reward: 17.0\n",
      "Total reward: 26.0\n",
      "Total reward: 83.0\n",
      "Total reward: 32.0\n",
      "Total reward: 71.0\n",
      "Total reward: 90.0\n",
      "Total reward: 75.0\n",
      "Total reward: 20.0\n",
      "Total reward: 84.0\n",
      "Total reward: 34.0\n",
      "Total reward: 132.0\n",
      "Total reward: 86.0\n",
      "Total reward: 87.0\n",
      "Total reward: 90.0\n",
      "Total reward: 126.0\n",
      "Total reward: 35.0\n",
      "Total reward: 83.0\n",
      "Total reward: 17.0\n",
      "Total reward: 103.0\n",
      "Total reward: 90.0\n",
      "Total reward: 160.0\n",
      "Total reward: 318.0\n",
      "Total reward: 19.0\n",
      "Total reward: 22.0\n",
      "Total reward: 70.0\n",
      "Total reward: 21.0\n",
      "Total reward: 39.0\n",
      "Total reward: 17.0\n",
      "Total reward: 35.0\n",
      "Total reward: 18.0\n",
      "Total reward: 38.0\n",
      "Total reward: 28.0\n",
      "Total reward: 19.0\n",
      "Total reward: 19.0\n",
      "Total reward: 27.0\n",
      "Total reward: 58.0\n",
      "Total reward: 25.0\n",
      "Total reward: 29.0\n",
      "Total reward: 29.0\n",
      "Total reward: 23.0\n",
      "Total reward: 43.0\n",
      "Total reward: 32.0\n",
      "Total reward: 57.0\n",
      "Total reward: 20.0\n",
      "Total reward: 27.0\n",
      "Total reward: 57.0\n",
      "Total reward: 20.0\n",
      "Total reward: 18.0\n",
      "Total reward: 30.0\n",
      "Total reward: 38.0\n",
      "Total reward: 48.0\n",
      "Total reward: 114.0\n",
      "Total reward: 95.0\n",
      "Total reward: 44.0\n",
      "Total reward: 113.0\n",
      "Total reward: 75.0\n",
      "Total reward: 27.0\n",
      "Total reward: 35.0\n",
      "Total reward: 34.0\n",
      "Total reward: 36.0\n",
      "Total reward: 31.0\n",
      "Total reward: 111.0\n",
      "Total reward: 116.0\n",
      "Total reward: 116.0\n",
      "Total reward: 57.0\n",
      "Total reward: 137.0\n",
      "Total reward: 128.0\n",
      "Total reward: 96.0\n",
      "Total reward: 100.0\n",
      "Total reward: 89.0\n",
      "Total reward: 120.0\n",
      "Total reward: 99.0\n",
      "Total reward: 139.0\n",
      "Total reward: 105.0\n",
      "Total reward: 38.0\n",
      "Total reward: 122.0\n",
      "Total reward: 99.0\n",
      "Total reward: 27.0\n",
      "Total reward: 45.0\n",
      "Total reward: 92.0\n",
      "Total reward: 135.0\n",
      "Total reward: 111.0\n",
      "Total reward: 181.0\n",
      "Total reward: 47.0\n",
      "Total reward: 124.0\n",
      "Total reward: 83.0\n",
      "Total reward: 108.0\n",
      "Total reward: 147.0\n",
      "Total reward: 123.0\n",
      "Total reward: 143.0\n",
      "Total reward: 307.0\n",
      "Total reward: 322.0\n",
      "Total reward: 137.0\n",
      "Total reward: 218.0\n",
      "Total reward: 151.0\n",
      "Total reward: 113.0\n",
      "Total reward: 273.0\n",
      "Total reward: 137.0\n",
      "Total reward: 117.0\n",
      "Total reward: 168.0\n",
      "Total reward: 47.0\n",
      "Total reward: 103.0\n",
      "Total reward: 131.0\n",
      "Total reward: 125.0\n",
      "Total reward: 97.0\n",
      "Total reward: 137.0\n",
      "Total reward: 42.0\n",
      "Total reward: 92.0\n",
      "Total reward: 18.0\n",
      "Total reward: 58.0\n",
      "Total reward: 112.0\n",
      "Total reward: 131.0\n",
      "Total reward: 121.0\n",
      "Total reward: 102.0\n",
      "Total reward: 35.0\n",
      "Total reward: 20.0\n",
      "Total reward: 53.0\n",
      "Total reward: 16.0\n",
      "Total reward: 45.0\n",
      "Total reward: 81.0\n",
      "Total reward: 143.0\n",
      "Total reward: 114.0\n",
      "Total reward: 109.0\n",
      "Total reward: 134.0\n",
      "Total reward: 108.0\n",
      "Total reward: 16.0\n",
      "Total reward: 97.0\n",
      "Total reward: 107.0\n",
      "Total reward: 113.0\n",
      "Total reward: 113.0\n",
      "Total reward: 103.0\n",
      "Total reward: 105.0\n",
      "Total reward: 121.0\n",
      "Total reward: 102.0\n",
      "Total reward: 17.0\n",
      "Total reward: 110.0\n",
      "Total reward: 119.0\n",
      "Total reward: 98.0\n",
      "Total reward: 114.0\n",
      "Total reward: 107.0\n",
      "Total reward: 31.0\n",
      "Total reward: 115.0\n",
      "Total reward: 132.0\n",
      "Total reward: 44.0\n",
      "Total reward: 16.0\n",
      "Total reward: 16.0\n",
      "Total reward: 117.0\n",
      "Total reward: 139.0\n",
      "Total reward: 18.0\n",
      "Total reward: 21.0\n",
      "Total reward: 16.0\n",
      "Total reward: 95.0\n",
      "Total reward: 95.0\n",
      "Total reward: 128.0\n",
      "Total reward: 101.0\n",
      "Total reward: 15.0\n",
      "Total reward: 16.0\n",
      "Total reward: 113.0\n",
      "Total reward: 27.0\n",
      "Total reward: 15.0\n",
      "Total reward: 16.0\n",
      "Total reward: 17.0\n",
      "Total reward: 90.0\n",
      "Total reward: 13.0\n",
      "Total reward: 100.0\n",
      "Total reward: 15.0\n",
      "Total reward: 15.0\n",
      "Total reward: 107.0\n",
      "Total reward: 20.0\n",
      "Total reward: 95.0\n",
      "Total reward: 14.0\n",
      "Total reward: 99.0\n",
      "Total reward: 111.0\n",
      "Total reward: 35.0\n",
      "Total reward: 106.0\n",
      "Total reward: 102.0\n",
      "Total reward: 21.0\n",
      "Total reward: 16.0\n",
      "Total reward: 108.0\n",
      "Total reward: 126.0\n",
      "Total reward: 120.0\n",
      "Total reward: 105.0\n",
      "Total reward: 21.0\n",
      "Total reward: 20.0\n",
      "Total reward: 15.0\n",
      "Total reward: 105.0\n",
      "Total reward: 18.0\n",
      "Total reward: 109.0\n",
      "Total reward: 120.0\n",
      "Total reward: 61.0\n",
      "Total reward: 99.0\n",
      "Total reward: 43.0\n",
      "Total reward: 123.0\n",
      "Total reward: 105.0\n",
      "Total reward: 108.0\n",
      "Total reward: 97.0\n",
      "Total reward: 104.0\n",
      "Total reward: 117.0\n",
      "Total reward: 156.0\n",
      "Total reward: 110.0\n",
      "Total reward: 102.0\n",
      "Total reward: 103.0\n",
      "Total reward: 112.0\n",
      "Total reward: 103.0\n",
      "Total reward: 104.0\n",
      "Total reward: 107.0\n",
      "Total reward: 118.0\n",
      "Total reward: 217.0\n",
      "Total reward: 307.0\n",
      "Total reward: 149.0\n",
      "Total reward: 220.0\n",
      "Total reward: 34.0\n",
      "Total reward: 23.0\n",
      "Total reward: 153.0\n",
      "Total reward: 84.0\n",
      "Total reward: 46.0\n",
      "Total reward: 170.0\n",
      "Total reward: 26.0\n",
      "Total reward: 220.0\n",
      "Total reward: 70.0\n",
      "Total reward: 189.0\n",
      "Total reward: 158.0\n",
      "Total reward: 212.0\n",
      "Total reward: 141.0\n",
      "Total reward: 199.0\n",
      "Total reward: 176.0\n",
      "Total reward: 115.0\n",
      "Total reward: 42.0\n",
      "Total reward: 134.0\n",
      "Total reward: 340.0\n",
      "Total reward: 333.0\n",
      "Total reward: 279.0\n",
      "Total reward: 173.0\n",
      "Total reward: 117.0\n",
      "Total reward: 164.0\n",
      "Total reward: 127.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-230fc53070a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Train the nnet that approximates q(s,a), using the replay memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mnnet_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Decrease epsilon until we hit a target threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m:\u001b[0m      \u001b[0mepsilon\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-dc1281635bfe>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(replay_memory, minibatch_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mminibatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0ms_l\u001b[0m \u001b[0;34m=\u001b[0m      \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0ma_l\u001b[0m \u001b[0;34m=\u001b[0m      \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mr_l\u001b[0m \u001b[0;34m=\u001b[0m      \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[1;32m   2476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2478\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NoValue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NoValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2479\u001b[0m     \"\"\"\n\u001b[1;32m   2480\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproduct\u001b[0m \u001b[0mof\u001b[0m \u001b[0marray\u001b[0m \u001b[0melements\u001b[0m \u001b[0mover\u001b[0m \u001b[0ma\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "steps = 0 \n",
    "for n in range(n_episodes): \n",
    "    s = env.reset()\n",
    "    done=False\n",
    "    r_sum = 0\n",
    "    while not done: \n",
    "        # Uncomment this to see the agent learning\n",
    "        env.render()\n",
    "        # Feedforward pass for current state to get predicted q-values for all actions \n",
    "        qvals_s = nnet_q.predict(s.reshape(1,4))\n",
    "        # Choose action to be epsilon-greedy\n",
    "        if np.random.random() < epsilon:  a = env.action_space.sample()\n",
    "        else:                             a = np.argmax(qvals_s); \n",
    "        # Take step, store results \n",
    "        sprime, r, done, info = env.step(a)\n",
    "        r_sum += r \n",
    "        # add to memory, respecting memory buffer limit \n",
    "        if len(replay_memory) > mem_max_size:\n",
    "            replay_memory.pop(0)\n",
    "        replay_memory.append({\"s\":s,\"a\":a,\"r\":r,\"sprime\":sprime,\"done\":done})\n",
    "        # Update target weights every C steps \n",
    "        steps +=1 \n",
    "        if steps % C == 0: nnet_target.set_weights(nnet_q.get_weights())\n",
    "            \n",
    "        # Update state\n",
    "        s=sprime\n",
    "        \n",
    "        # Train the nnet that approximates q(s,a), using the replay memory\n",
    "        nnet_q = replay(replay_memory, minibatch_size = minibatch_size)\n",
    "        # Decrease epsilon until we hit a target threshold \n",
    "        if epsilon > 0.01:      epsilon -= 0.001\n",
    "    print(\"Total reward:\", r_sum)\n",
    "    r_sums.append(r_sum)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_sum.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
